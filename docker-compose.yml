name: "spark-ecosystem-cluster"

networks:
  spark-kafka:
    name: spark-kafka
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: "172.18.0.0/16"

x-shared-volumes:
  &spark-volumes
  - ./apps:/opt/spark-apps
  - ./data:/opt/spark-data
  - ./spark/metrics.properties:/opt/spark/conf/metrics.properties
  - ./spark/config/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf

services:
  spark-master:
    build:
      context: ./spark
      dockerfile: Dockerfile
    ports:
      - "9090:8080"
      - "7077:7077"
      - "4040:4040"
    volumes: *spark-volumes
    environment:
      - SPARK_LOCAL_IP=172.18.0.10
      - SPARK_WORKLOAD=master
      - PYSPARK_PYTHON=python3
      - SPARK_JARS_PACKAGES=org.apache.hadoop:hadoop-aws:3.3.1,com.amazonaws:aws-java-sdk-bundle:1.11.901

    networks:
      spark-kafka:
        ipv4_address: 172.18.0.10

  spark-worker-a:
    build:
      context: ./spark
      dockerfile: Dockerfile
    ports:
      - "9091:8080"
      - "7000:7000"
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2G
      - SPARK_EXECUTOR_MEMORY=2G
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=172.18.0.2
      - PYSPARK_PYTHON=python3
      - SPARK_JARS_PACKAGES=org.apache.hadoop:hadoop-aws:3.3.1,com.amazonaws:aws-java-sdk-bundle:1.11.901
    volumes: *spark-volumes
    networks:
      spark-kafka:
        ipv4_address: 172.18.0.2

  spark-worker-b:
    build:
      context: ./spark
      dockerfile: Dockerfile
    ports:
      - "9093:8080"
      - "7001:7000"
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2G
      - SPARK_EXECUTOR_MEMORY=2G
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=172.18.0.3
      - PYSPARK_PYTHON=python3
      - SPARK_JARS_PACKAGES=org.apache.hadoop:hadoop-aws:3.3.1,com.amazonaws:aws-java-sdk-bundle:1.11.901
    volumes: *spark-volumes
    networks:
      spark-kafka:
        ipv4_address: 172.18.0.3

  kafka:
    image: bitnami/kafka:3.7.0
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_CFG_NODE_ID: 0
      KAFKA_CFG_PROCESS_ROLES: controller,broker
      KAFKA_CFG_LISTENERS: "PLAINTEXT://172.18.0.4:9092,CONTROLLER://0.0.0.0:9093"
      KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: "CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT"
      KAFKA_CFG_CONTROLLER_QUORUM_VOTERS: 0@172.18.0.4:9093
      KAFKA_CFG_CONTROLLER_LISTENER_NAMES: CONTROLLER
    networks:
      spark-kafka:
        ipv4_address: 172.18.0.4

  init-kafka:
    image: bitnami/kafka:3.7.0
    container_name: init-kafka
    depends_on:
      kafka:
        condition: service_started
    entrypoint: [ '/usr/bin/bash', '-c' ]
    command: |
      "
      set -ex

      # blocks until kafka is reachable
      /opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server 172.18.0.4:9092 --list
  
      echo -e 'Creating kafka topics'
      /opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server 172.18.0.4:9092 --create --if-not-exists --topic topic-events --replication-factor 1 --partitions 1

      echo -e 'Successfully created the following topics:'
      /opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server 172.18.0.4:9092 --list
      "
    networks:
      spark-kafka:
        ipv4_address: 172.18.0.21

  localstack:
    container_name: "${LOCALSTACK_DOCKER_NAME:-localstack-main}"
    image: localstack/localstack:3.4.0
    ports:
      - "127.0.0.1:4566:4566"
      - "127.0.0.1:4510-4559:4510-4559"
    environment:
      - SERVICES=events,sqs,s3
      - DEBUG=true
      - DEFAULT_REGION=eu-west-1
      - AWS_ACCESS_KEY_ID=test
      - AWS_SECRET_ACCESS_KEY=test
    volumes:
      - "${LOCALSTACK_VOLUME_DIR:-./volume}:/var/lib/localstack"
      - "/var/run/docker.sock:/var/run/docker.sock"
    networks:
      spark-kafka:
        ipv4_address: 172.18.0.6

  init-s3-storage:
    container_name: init-s3-storage
    image: localstack/localstack:3.4.0
    entrypoint: [ "bash", "-c", "awslocal --endpoint-url http://172.18.0.6:4566 s3 mb s3://my-bucket" ]
    depends_on:
      localstack:
        condition: service_healthy
    networks:
      spark-kafka:
        ipv4_address: 172.18.0.7

  rabbitmq:
    image: 'rabbitmq:3.12-management'
    restart: unless-stopped
    ports:
      - "5672:5672"
      - "15672:15672"
    environment:
      - RABBITMQ_DEFAULT_USER=codely
      - RABBITMQ_DEFAULT_PASS=codely
    networks:
      spark-kafka:
        ipv4_address: 172.18.0.20

  postgres:
    image: postgres:14-alpine
    container_name: postgres
    environment:
      - POSTGRES_USER
      - POSTGRES_PASSWORD
      - POSTGRES_DB
    healthcheck:
      test: [ "CMD", "psql", "-U", "${POSTGRES_USER}", "${POSTGRES_DB}" ]
    ports:
      - '5432:5432'
    networks:
      spark-kafka:
        ipv4_address: 172.18.0.8

  hive-metastore:
    image: apache/hive:4.0.0-alpha-2
    container_name: hive-metastore
    environment:
      - SERVICE_NAME=metastore
      - DB_DRIVER=postgres
      - HIVE_CUSTOM_CONF_DIR=/hive_custom_conf
    ports:
      - "9083:9083"
    volumes:
      - ./data/delta/osdp/spark-warehouse:/opt/spark/work-dir/data/delta/osdp/spark-warehouse
      - ./hive/conf/hive-site.xml:/opt/hive/conf/hive-site.xml
      - ./hive/conf/jars/hadoop-aws-3.2.2.jar:/opt/hive/lib/hadoop-aws-3.2.2.jar
      - ./hive/conf/jars/aws-java-sdk-bundle-1.11.375.jar:/opt/hive/lib/aws-java-sdk-bundle-1.11.375.jar
      - ./hive/conf/.hiverc:/opt/hive/conf/.hiverc
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      spark-kafka:
        ipv4_address: 172.18.0.9

#  spark-thrift-server:
#    image: my-spark-cluster:3.5.0
#    container_name: spark-thrift-server
#    depends_on:
#      spark-master:
#        condition: service_started
#      hive-metastore:
#        condition: service_started
#    environment:
#      - SPARK_WORKLOAD=thrift-server
#      - HIVE_METASTORE_URI=thrift://172.18.0.8:9083
#      - SPARK_MASTER=spark://172.18.0.10:7077
#      - SPARK_SQL_HIVE_METASTORE_URIS=thrift://172.18.0.8:9083
#    ports:
#      - "10000:10000"
#    entrypoint: >
#      /opt/spark/bin/spark-submit
#        --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2
#        --master spark://spark-master:7077
#        --deploy-mode client
#        --executor-memory 512M
#        --driver-memory 512M
#        --conf spark.executor.cores=1
#        --conf spark.executor.instances=1
#        --conf spark.dynamicAllocation.enabled=false
#        --conf spark.cores.max=2
#        --conf spark.sql.hive.metastore.version=2.3.9
#        --conf spark.sql.uris=thrift://172.18.0.9:9083
#        --conf spark.sql.hive.metastore.jars=maven
#        --conf spark.hadoop.hive.metastore.uris=thrift://172.18.0.9:9083
#        --conf spark.hadoop.fs.s3a.endpoint=http://172.18.0.6:4566
#        --conf spark.hadoop.fs.s3a.access.key=test
#        --conf spark.hadoop.fs.s3a.secret.key=test
#        --conf spark.hadoop.fs.s3a.path.style.access=true
#        --packages org.apache.hadoop:hadoop-aws:3.3.1,com.amazonaws:aws-java-sdk-bundle:1.11.375
#        local://opt/spark/jars/spark-hive-thriftserver_2.12-3.5.0.jar
#    networks:
#      spark-kafka:
#        ipv4_address: 172.18.0.11

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    networks:
      spark-kafka:
        ipv4_address: 172.18.0.13

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "19090:9090"
    networks:
      spark-kafka:
        ipv4_address: 172.18.0.12
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml

  jupyter:
    image: jupyter/pyspark-notebook:latest
    container_name: jupyter
    ports:
      - "8888:8888"
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_HOME=/usr/local/spark
      - PYSPARK_PYTHON=python3
    volumes:
      - ./notebooks:/home/jovyan/work
      - ./apps:/opt/spark-apps
      - ./data:/opt/spark-data
      - ./spark/config/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
    depends_on:
      - spark-master
      - kafka
    networks:
      spark-kafka:
        ipv4_address: 172.18.0.15
    command: >
      start.sh jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser --allow-root --NotebookApp.token='' --NotebookApp.password=''

  airflow-postgres:
    image: postgres:12
    container_name: airflow-postgres
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    ports:
      - "5434:5432"
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U airflow" ]
      interval: 10s
      retries: 5
    networks:
      spark-kafka:
        ipv4_address: 172.18.0.30

  airflow-init:
    image: apache/airflow:2.6.2
    container_name: airflow_init
    depends_on:
      airflow-postgres:
        condition: service_healthy
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@172.18.0.30:5432/airflow
      - AIRFLOW__CORE__FERNET_KEY=FB0o_zt4e3Ziq3LdUUO7F2Z95cvFFx16hU8jTeR1ASM=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__LOGGING_LEVEL=INFO
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db init &&
        airflow users create \
          --role Admin \
          --username airflow \
          --password airflow \
          --email airflow@airflow.com \
          --firstname airflow \
          --lastname airflow
    networks:
      spark-kafka:
        ipv4_address: 172.18.0.31

  airflow-webserver:
    image: apache/airflow:2.6.2
    container_name: airflow_webserver
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    ports:
      - "8080:8080"
    volumes: # sudo chmod 777 -R ./airflow-data
      - ./airflow-data:/opt/airflow/dags
      - ./airflow-data/logs:/opt/airflow/logs
      - ./airflow-data/logs:/opt/airflow/logs
      - ./airflow-data/plugins:/opt/airflow/plugins
      - ./airflow-data/config:/opt/airflow/config
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@172.18.0.30:5432/airflow
      - AIRFLOW__CORE__FERNET_KEY=FB0o_zt4e3Ziq3LdUUO7F2Z95cvFFx16hU8jTeR1ASM=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__LOGGING_LEVEL=INFO
    command: airflow webserver
    healthcheck:
      test: [ "CMD", "curl", "--fail", "http://localhost:8080/health" ]
      interval: 30s
      timeout: 30s
      retries: 5
    networks:
      spark-kafka:
        ipv4_address: 172.18.0.32

  airflow-scheduler:
    image: apache/airflow:2.6.2
    container_name: airflow_scheduler

  superset:
    init: true
    build:
      context: ./superset
      dockerfile: Dockerfile
    container_name: superset
    volumes:
      - ./superset_home:/app/superset_home
    environment:
      - DATABASE_DB=superset
      - DATABASE_HOST=superset-database
      - DATABASE_PASSWORD=secretsecret
      - DATABASE_USER=superset
      - DATABASE_PORT=5432
    ports:
      - '8081:8088'
    depends_on:
      superset-database:
        condition: service_healthy
    networks:
      spark-kafka:
        ipv4_address: 172.18.0.33

  superset-database:
    init: true
    image: postgres:alpine
    container_name: superset_db
    volumes:
      - ./postgres_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_DB=superset
      - POSTGRES_USER=superset
      - POSTGRES_PASSWORD=secretsecret
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U superset" ]
      interval: 10s
      timeout: 5s
      retries: 5
    ports:
      - '5000:5432'
    networks:
      spark-kafka:
        ipv4_address: 172.18.0.34

volumes:
  warehouse: